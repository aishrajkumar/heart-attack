{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECS 171 Group 12 Heart Attack Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trtbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalachh</th>\n",
       "      <th>exng</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slp</th>\n",
       "      <th>caa</th>\n",
       "      <th>thall</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n",
       "298   57    0   0     140   241    0        1       123     1      0.2    1   \n",
       "299   45    1   3     110   264    0        1       132     0      1.2    1   \n",
       "300   68    1   0     144   193    1        1       141     0      3.4    1   \n",
       "301   57    1   0     130   131    0        1       115     1      1.2    1   \n",
       "302   57    0   1     130   236    0        0       174     0      0.0    1   \n",
       "\n",
       "     caa  thall  output  \n",
       "298    0      3       0  \n",
       "299    0      3       0  \n",
       "300    2      3       0  \n",
       "301    1      3       0  \n",
       "302    1      2       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in dataframe\n",
    "df = pd.read_csv('./heart.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Outlier Factor Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Outlier Factor: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "LOF = LocalOutlierFactor(n_neighbors = 20, contamination = \"auto\")\n",
    "\n",
    "lof_outliers = LOF.fit_predict(df)\n",
    "print(\"Local Outlier Factor: \" + str(lof_outliers[np.where(lof_outliers == -1)].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_list = np.where(lof_outliers == -1)[0].tolist()\n",
    "df = df.drop(outlier_list)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VIF Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop = df.drop([\"trtbps\", \"thalachh\", \"age\",\"chol\",\"thall\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = MinMaxScaler()\n",
    "\n",
    "X_drop = df_drop.drop(\"output\", axis = 1)\n",
    "y_drop = df_drop[\"output\"]\n",
    "\n",
    "X = df.drop(\"output\", axis = 1)\n",
    "y = df[\"output\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_drop, X_test_drop, y_train_drop, y_test_drop = train_test_split(X_drop, y_drop,test_size = 0.2, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier, HistGradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.metrics import recall_score,precision_score, f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from xgboost import XGBClassifier,XGBRFClassifier\n",
    "\n",
    "\n",
    "def model_selection(X_train, y_train,X_test, y_test, estimator):\n",
    "    \"\"\"\n",
    "    Test various estimators.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = estimator\n",
    "\n",
    "    # Instantiate the classification model and visualizer\n",
    "    model.fit(X_train, y_train)  \n",
    "    \n",
    "    expected  = y_test\n",
    "    predicted = model.predict(X_test)\n",
    "    \n",
    "\n",
    "    # Compute and return the F1 score (the harmonic mean of precision and recall)\n",
    "    return [model.score(X_test,y_test), \n",
    "            recall_score(expected, predicted),\n",
    "            precision_score(expected, predicted),\n",
    "            f1_score(expected, predicted)]\n",
    "\n",
    "def model_selection_normalize(X_train, y_train,X_test, y_test, estimator):\n",
    "    \"\"\"\n",
    "    Test various estimators.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = estimator\n",
    "\n",
    "#     Instantiate the classification model and visualizer\n",
    "    model.fit(normalize.fit_transform(X_train), y_train)  \n",
    "    \n",
    "    expected  = y_test\n",
    "    predicted = model.predict(normalize.fit_transform(X_test))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Compute and return the F1 score (the harmonic mean of precision and recall)\n",
    "    return [model.score(normalize.fit_transform(X_test),y_test), \n",
    "            recall_score(expected, predicted),\n",
    "            precision_score(expected, predicted),\n",
    "            f1_score(expected, predicted)\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considered Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_lst = [\n",
    "                 RandomForestClassifier(random_state = 42),\n",
    "                 SVC(random_state = 42),\n",
    "                 LogisticRegression(random_state = 42),\n",
    "                 MLPClassifier(random_state = 42),\n",
    "                 GaussianNB(),\n",
    "                 KNeighborsClassifier(),\n",
    "                 SGDClassifier(random_state = 42),\n",
    "                 PassiveAggressiveClassifier(random_state = 42),\n",
    "                 GradientBoostingClassifier(random_state = 42),\n",
    "                 AdaBoostClassifier(random_state = 42),\n",
    "                 HistGradientBoostingClassifier(random_state = 42),\n",
    "                 GaussianProcessClassifier(random_state = 42),\n",
    "                 BaggingClassifier(random_state = 42),\n",
    "                 XGBClassifier(random_state = 42,use_label_encoder=False),\n",
    "                 XGBRFClassifier(random_state = 42, use_label_encoder=False),\n",
    "                \n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF with all columns not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns\n",
      "not normalized\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8157894736842105\n",
      "\n",
      "F1 Score: 0.861111111111111\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6610169491525424\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.6590909090909091\n",
      "\n",
      "F1 Score: 0.7435897435897436\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.7777777777777778\n",
      "\n",
      "F1 Score: 0.7999999999999999\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.6440677966101694\n",
      "\n",
      "Recall: 0.6470588235294118\n",
      "\n",
      "Precision: 0.7096774193548387\n",
      "\n",
      "F1 Score: 0.6769230769230768\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6949152542372882\n",
      "\n",
      "Recall: 0.6470588235294118\n",
      "\n",
      "Precision: 0.7857142857142857\n",
      "\n",
      "F1 Score: 0.7096774193548386\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.4406779661016949\n",
      "\n",
      "Recall: 0.029411764705882353\n",
      "\n",
      "Precision: 1.0\n",
      "\n",
      "F1 Score: 0.05714285714285715\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.7567567567567568\n",
      "\n",
      "F1 Score: 0.7887323943661971\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7435897435897436\n",
      "\n",
      "F1 Score: 0.7945205479452054\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6101694915254238\n",
      "\n",
      "Recall: 0.5882352941176471\n",
      "\n",
      "Precision: 0.6896551724137931\n",
      "\n",
      "F1 Score: 0.6349206349206349\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7288135593220338\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.725\n",
      "\n",
      "F1 Score: 0.7837837837837837\n",
      "\n",
      "[11:26:22] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7317073170731707\n",
      "\n",
      "F1 Score: 0.8\n",
      "\n",
      "[11:26:22] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.75\n",
      "\n",
      "F1 Score: 0.8108108108108107\n",
      "\n",
      "-----------------------\n",
      "Top Model:  LogisticRegression(random_state=42)\n",
      "Top F1 Score:  0.8648648648648648\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns\")\n",
    "print(\"not normalized\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection(X_train, y_train, X_test, y_test, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score: \", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF with all columns normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns\n",
      "normalized\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8157894736842105\n",
      "\n",
      "F1 Score: 0.861111111111111\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.7727272727272727\n",
      "\n",
      "F1 Score: 0.8717948717948718\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.7857142857142857\n",
      "\n",
      "F1 Score: 0.8684210526315789\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.7857142857142857\n",
      "\n",
      "F1 Score: 0.8684210526315789\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8115942028985507\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8205128205128205\n",
      "\n",
      "F1 Score: 0.8767123287671232\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.6938775510204082\n",
      "\n",
      "F1 Score: 0.819277108433735\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7560975609756098\n",
      "\n",
      "F1 Score: 0.8266666666666665\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.8285714285714286\n",
      "\n",
      "F1 Score: 0.8405797101449276\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8333333333333334\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.8055555555555556\n",
      "\n",
      "F1 Score: 0.8285714285714286\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.8048780487804879\n",
      "\n",
      "F1 Score: 0.8800000000000001\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7288135593220338\n",
      "\n",
      "Recall: 0.7352941176470589\n",
      "\n",
      "Precision: 0.78125\n",
      "\n",
      "F1 Score: 0.7575757575757576\n",
      "\n",
      "[11:26:24] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n",
      "[11:26:24] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7435897435897436\n",
      "\n",
      "F1 Score: 0.7945205479452054\n",
      "\n",
      "-----------------------\n",
      "Top Model:  GaussianProcessClassifier(random_state=42)\n",
      "Top F1 Score:  0.8800000000000001\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns\")\n",
    "print(\"normalized\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection_normalize(X_train, y_train, X_test, y_test, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score: \", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF with columns removed not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns:\n",
      "not normalized:\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8205128205128205\n",
      "\n",
      "F1 Score: 0.8767123287671232\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.864406779661017\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.8095238095238095\n",
      "\n",
      "F1 Score: 0.8947368421052632\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.8048780487804879\n",
      "\n",
      "F1 Score: 0.8800000000000001\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7837837837837838\n",
      "\n",
      "F1 Score: 0.8169014084507041\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.8055555555555556\n",
      "\n",
      "F1 Score: 0.8285714285714286\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.7727272727272727\n",
      "\n",
      "F1 Score: 0.8717948717948718\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.7111111111111111\n",
      "\n",
      "F1 Score: 0.8101265822784811\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.864406779661017\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.825\n",
      "\n",
      "F1 Score: 0.8918918918918919\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.7906976744186046\n",
      "\n",
      "F1 Score: 0.8831168831168831\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.864406779661017\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.8095238095238095\n",
      "\n",
      "F1 Score: 0.8947368421052632\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8205128205128205\n",
      "\n",
      "F1 Score: 0.8767123287671232\n",
      "\n",
      "[11:26:27] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.7857142857142857\n",
      "\n",
      "F1 Score: 0.8684210526315789\n",
      "\n",
      "[11:26:27] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8813559322033898\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.8292682926829268\n",
      "\n",
      "F1 Score: 0.9066666666666667\n",
      "\n",
      "-----------------------\n",
      "Top Model:  XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "Top F1 Score 0.9066666666666667\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns:\")\n",
    "print(\"not normalized:\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection(X_train_drop, y_train_drop, X_test_drop, y_test_drop, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF with columns removed normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns:\n",
      "normalized:\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8205128205128205\n",
      "\n",
      "F1 Score: 0.8767123287671232\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.8048780487804879\n",
      "\n",
      "F1 Score: 0.8800000000000001\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.7777777777777778\n",
      "\n",
      "F1 Score: 0.7999999999999999\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8115942028985507\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.775\n",
      "\n",
      "F1 Score: 0.8378378378378379\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8108108108108109\n",
      "\n",
      "F1 Score: 0.8450704225352113\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8157894736842105\n",
      "\n",
      "F1 Score: 0.861111111111111\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8378378378378378\n",
      "\n",
      "F1 Score: 0.8732394366197184\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.8048780487804879\n",
      "\n",
      "F1 Score: 0.8800000000000001\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.864406779661017\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8421052631578947\n",
      "\n",
      "F1 Score: 0.8888888888888888\n",
      "\n",
      "[11:26:28] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8378378378378378\n",
      "\n",
      "F1 Score: 0.8732394366197184\n",
      "\n",
      "[11:26:29] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.8048780487804879\n",
      "\n",
      "F1 Score: 0.8800000000000001\n",
      "\n",
      "-----------------------\n",
      "Top Model:  BaggingClassifier(random_state=42)\n",
      "Top F1 Score 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns:\")\n",
    "print(\"normalized:\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection_normalize(X_train_drop, y_train_drop, X_test_drop, y_test_drop, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "# cross val \n",
    "X_resample_drop = pd.concat([X_train_drop, y_train_drop], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "no_heart_attack_drop = X_resample_drop[X_resample_drop.output==0]\n",
    "heart_attack_drop = X_resample_drop[X_resample_drop.output==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resample = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "no_heart_attack= X_resample[X_resample.output==0]\n",
    "heart_attack= X_resample[X_resample.output==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsampling Dropped Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample minority\n",
    "output_upsample_drop = resample(no_heart_attack_drop,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(heart_attack_drop), # match number in majority class\n",
    "                          random_state=42) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsample_drop = pd.concat([heart_attack_drop, output_upsample_drop])\n",
    "\n",
    "X_train_drop_upsample = upsample_drop.drop(\"output\", axis = 1)\n",
    "y_train_drop_upsample = upsample_drop[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling Dropped Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample minority\n",
    "output_downsample_drop = resample(heart_attack_drop,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(no_heart_attack_drop), # match number in majority class\n",
    "                          random_state=42) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "downsample_drop = pd.concat([output_downsample_drop, no_heart_attack_drop])\n",
    "\n",
    "X_train_drop_downsample = downsample_drop.drop(\"output\", axis = 1)\n",
    "y_train_drop_downsample = downsample_drop[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsampling With All Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample minority\n",
    "output_upsample = resample(no_heart_attack,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(heart_attack), # match number in majority class\n",
    "                          random_state=42) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsample = pd.concat([heart_attack, output_upsample])\n",
    "X_train_upsample = upsample.drop(\"output\", axis = 1)\n",
    "y_train_upsample = upsample[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling With All Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample majority\n",
    "output_downsample = resample(heart_attack,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(no_heart_attack), # match minority n\n",
    "                                random_state = 42) # reproducible results\n",
    "\n",
    "# combine minority and downsampled majority\n",
    "downsample = pd.concat([output_downsample, no_heart_attack])\n",
    "X_train_downsample = downsample.drop(\"output\", axis = 1)\n",
    "y_train_downsample = downsample[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_resample.drop(\"output\", axis = 1), X_resample[\"output\"])\n",
    "X_train_smote = X_train_smote[ ['age'] + [ col for col in X_train_smote.columns if col != 'age' ] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smote, sex_smote = sm.fit_resample(X_resample.drop(\"sex\", axis = 1), X_resample[\"sex\"])\n",
    "X_smote[\"sex\"] = sex_smote.values\n",
    "\n",
    "X_train_smote_sex = X_smote.drop(\"output\", axis = 1)\n",
    "smote_cols_sex = X_train_smote_sex.columns.tolist()\n",
    "smote_cols_sex = smote_cols_sex[-1:]  + smote_cols_sex[:-1]\n",
    "X_train_smote_sex = X_train_smote_sex[smote_cols_sex]\n",
    "X_train_smote_sex = X_train_smote_sex[ ['age'] + [ col for col in X_train_smote_sex.columns if col != 'age' ] ]\n",
    "y_train_smote_sex = X_smote[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smote_drop, sex_smote_drop = sm.fit_resample(X_resample_drop.drop(\"sex\", axis = 1), X_resample_drop[\"sex\"])\n",
    "X_smote_drop[\"sex\"] = sex_smote_drop.values\n",
    "smote_drop = X_smote_drop\n",
    "\n",
    "X_train_drop_smote = smote_drop.drop(\"output\", axis = 1)\n",
    "smote_drop_cols = X_train_drop_smote.columns.tolist()\n",
    "smote_drop_cols = smote_drop_cols[-1:]  + smote_drop_cols[:-1]\n",
    "X_train_drop_smote_sex = X_train_drop_smote[smote_drop_cols]\n",
    "y_train_drop_smote_sex = smote_drop[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_drop_smote, y_train_drop_smote = sm.fit_resample(X_resample_drop.drop(\"output\", axis = 1), X_resample_drop[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropped Columns Not Normalized Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns:\n",
      "not normalized:\n",
      "upsample\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8205128205128205\n",
      "\n",
      "F1 Score: 0.8767123287671232\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.864406779661017\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.8095238095238095\n",
      "\n",
      "F1 Score: 0.8947368421052632\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7837837837837838\n",
      "\n",
      "F1 Score: 0.8169014084507041\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8108108108108109\n",
      "\n",
      "F1 Score: 0.8450704225352113\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6610169491525424\n",
      "\n",
      "Recall: 0.5294117647058824\n",
      "\n",
      "Precision: 0.8181818181818182\n",
      "\n",
      "F1 Score: 0.6428571428571428\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.7941176470588235\n",
      "\n",
      "Precision: 0.84375\n",
      "\n",
      "F1 Score: 0.8181818181818182\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.7804878048780488\n",
      "\n",
      "F1 Score: 0.8533333333333334\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.7857142857142857\n",
      "\n",
      "F1 Score: 0.8684210526315789\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7560975609756098\n",
      "\n",
      "F1 Score: 0.8266666666666665\n",
      "\n",
      "[11:26:32] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.7804878048780488\n",
      "\n",
      "F1 Score: 0.8533333333333334\n",
      "\n",
      "[11:26:33] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.775\n",
      "\n",
      "F1 Score: 0.8378378378378379\n",
      "\n",
      "-----------------------\n",
      "Top Model:  SVC(random_state=42)\n",
      "Top F1 Score 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns:\")\n",
    "print(\"not normalized:\")\n",
    "print(\"upsample\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection(X_train_drop_upsample, y_train_drop_upsample, X_test_drop, y_test_drop, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropped Columns Normalized Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns:\n",
      "normalized:\n",
      "upsample\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8157894736842105\n",
      "\n",
      "F1 Score: 0.861111111111111\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7837837837837838\n",
      "\n",
      "F1 Score: 0.8169014084507041\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7692307692307693\n",
      "\n",
      "F1 Score: 0.8219178082191781\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 0.7941176470588235\n",
      "\n",
      "Precision: 0.7714285714285715\n",
      "\n",
      "F1 Score: 0.782608695652174\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.7941176470588235\n",
      "\n",
      "Precision: 0.7941176470588235\n",
      "\n",
      "F1 Score: 0.7941176470588235\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8108108108108109\n",
      "\n",
      "F1 Score: 0.8450704225352113\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7692307692307693\n",
      "\n",
      "F1 Score: 0.8219178082191781\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8378378378378378\n",
      "\n",
      "F1 Score: 0.8732394366197184\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7317073170731707\n",
      "\n",
      "F1 Score: 0.8\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n",
      "[11:26:36] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8333333333333334\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "[11:26:36] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n",
      "-----------------------\n",
      "Top Model:  HistGradientBoostingClassifier(random_state=42)\n",
      "Top F1 Score 0.8732394366197184\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns:\")\n",
    "print(\"normalized:\")\n",
    "print(\"upsample\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection_normalize(X_train_drop_upsample, y_train_drop_upsample, X_test_drop, y_test_drop, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Dropped Columns Not Normalized Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df without dropped columns:\n",
      "not normalized:\n",
      "upsample\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7837837837837838\n",
      "\n",
      "F1 Score: 0.8169014084507041\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6610169491525424\n",
      "\n",
      "Recall: 0.7647058823529411\n",
      "\n",
      "Precision: 0.6842105263157895\n",
      "\n",
      "F1 Score: 0.7222222222222222\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7288135593220338\n",
      "\n",
      "Recall: 0.7058823529411765\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.7500000000000001\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.6271186440677966\n",
      "\n",
      "Recall: 0.7058823529411765\n",
      "\n",
      "Precision: 0.6666666666666666\n",
      "\n",
      "F1 Score: 0.6857142857142857\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7288135593220338\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.7368421052631579\n",
      "\n",
      "F1 Score: 0.7777777777777778\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.4915254237288136\n",
      "\n",
      "Recall: 0.11764705882352941\n",
      "\n",
      "Precision: 1.0\n",
      "\n",
      "F1 Score: 0.21052631578947367\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7288135593220338\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7045454545454546\n",
      "\n",
      "F1 Score: 0.794871794871795\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7288135593220338\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.7368421052631579\n",
      "\n",
      "F1 Score: 0.7777777777777778\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6610169491525424\n",
      "\n",
      "Recall: 0.7647058823529411\n",
      "\n",
      "Precision: 0.6842105263157895\n",
      "\n",
      "F1 Score: 0.7222222222222222\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7837837837837838\n",
      "\n",
      "F1 Score: 0.8169014084507041\n",
      "\n",
      "[11:26:38] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7288135593220338\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7142857142857143\n",
      "\n",
      "F1 Score: 0.7894736842105262\n",
      "\n",
      "[11:26:38] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7435897435897436\n",
      "\n",
      "F1 Score: 0.7945205479452054\n",
      "\n",
      "-----------------------\n",
      "Top Model:  GaussianNB()\n",
      "Top F1 Score 0.8493150684931507\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df without dropped columns:\")\n",
    "print(\"not normalized:\")\n",
    "print(\"upsample\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection(X_train_upsample, y_train_upsample, X_test, y_test, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Dropped Columns Normalized Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df without dropped columns:\n",
      "normalized:\n",
      "upsample\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.7272727272727273\n",
      "\n",
      "F1 Score: 0.8205128205128205\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.7777777777777778\n",
      "\n",
      "F1 Score: 0.7999999999999999\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7837837837837838\n",
      "\n",
      "F1 Score: 0.8169014084507041\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.7333333333333333\n",
      "\n",
      "F1 Score: 0.8354430379746834\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7837837837837838\n",
      "\n",
      "F1 Score: 0.8169014084507041\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7435897435897436\n",
      "\n",
      "F1 Score: 0.7945205479452054\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7837837837837838\n",
      "\n",
      "F1 Score: 0.8169014084507041\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7631578947368421\n",
      "\n",
      "F1 Score: 0.8055555555555555\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.7777777777777778\n",
      "\n",
      "F1 Score: 0.7999999999999999\n",
      "\n",
      "[11:26:41] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7380952380952381\n",
      "\n",
      "F1 Score: 0.8157894736842106\n",
      "\n",
      "[11:26:41] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.75\n",
      "\n",
      "F1 Score: 0.8108108108108107\n",
      "\n",
      "-----------------------\n",
      "Top Model:  MLPClassifier(random_state=42)\n",
      "Top F1 Score 0.8648648648648648\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df without dropped columns:\")\n",
    "print(\"normalized:\")\n",
    "print(\"upsample\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection_normalize(X_train_upsample, y_train_upsample, X_test, y_test, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropped Columns Not Normalized Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns:\n",
      "not normalized:\n",
      "downsample\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8108108108108109\n",
      "\n",
      "F1 Score: 0.8450704225352113\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.864406779661017\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.8095238095238095\n",
      "\n",
      "F1 Score: 0.8947368421052632\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.8048780487804879\n",
      "\n",
      "F1 Score: 0.8800000000000001\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.7288135593220338\n",
      "\n",
      "Recall: 0.7647058823529411\n",
      "\n",
      "Precision: 0.7647058823529411\n",
      "\n",
      "F1 Score: 0.7647058823529412\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8108108108108109\n",
      "\n",
      "F1 Score: 0.8450704225352113\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 0.7058823529411765\n",
      "\n",
      "Precision: 0.8275862068965517\n",
      "\n",
      "F1 Score: 0.7619047619047619\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 0.7352941176470589\n",
      "\n",
      "Precision: 0.8064516129032258\n",
      "\n",
      "F1 Score: 0.7692307692307693\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8108108108108109\n",
      "\n",
      "F1 Score: 0.8450704225352113\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8108108108108109\n",
      "\n",
      "F1 Score: 0.8450704225352113\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.8048780487804879\n",
      "\n",
      "F1 Score: 0.8800000000000001\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.8235294117647058\n",
      "\n",
      "F1 Score: 0.8235294117647058\n",
      "\n",
      "[11:26:43] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8333333333333334\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "[11:26:43] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.864406779661017\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.825\n",
      "\n",
      "F1 Score: 0.8918918918918919\n",
      "\n",
      "-----------------------\n",
      "Top Model:  SVC(random_state=42)\n",
      "Top F1 Score 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns:\")\n",
    "print(\"not normalized:\")\n",
    "print(\"downsample\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection(X_train_drop_downsample, y_train_drop_downsample, X_test_drop, y_test_drop, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropped Columns Normalized Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns:\n",
      "normalized:\n",
      "downsample\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.7777777777777778\n",
      "\n",
      "F1 Score: 0.7999999999999999\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.7288135593220338\n",
      "\n",
      "Recall: 0.7647058823529411\n",
      "\n",
      "Precision: 0.7647058823529411\n",
      "\n",
      "F1 Score: 0.7647058823529412\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.7288135593220338\n",
      "\n",
      "Recall: 0.7647058823529411\n",
      "\n",
      "Precision: 0.7647058823529411\n",
      "\n",
      "F1 Score: 0.7647058823529412\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 0.7058823529411765\n",
      "\n",
      "Precision: 0.8275862068965517\n",
      "\n",
      "F1 Score: 0.7619047619047619\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7837837837837838\n",
      "\n",
      "F1 Score: 0.8169014084507041\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8115942028985507\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8115942028985507\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8108108108108109\n",
      "\n",
      "F1 Score: 0.8450704225352113\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7837837837837838\n",
      "\n",
      "F1 Score: 0.8169014084507041\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.7941176470588235\n",
      "\n",
      "Precision: 0.7941176470588235\n",
      "\n",
      "F1 Score: 0.7941176470588235\n",
      "\n",
      "[11:26:45] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.8285714285714286\n",
      "\n",
      "F1 Score: 0.8405797101449276\n",
      "\n",
      "[11:26:46] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8378378378378378\n",
      "\n",
      "F1 Score: 0.8732394366197184\n",
      "\n",
      "-----------------------\n",
      "Top Model:  XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "Top F1 Score 0.8732394366197184\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns:\")\n",
    "print(\"normalized:\")\n",
    "print(\"downsample\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection_normalize(X_train_drop_downsample, y_train_drop_downsample, X_test_drop, y_test_drop, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Dropped Columns Not Normalized Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df without dropped columns:\n",
      "not normalized:\n",
      "downsample\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8157894736842105\n",
      "\n",
      "F1 Score: 0.861111111111111\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6440677966101694\n",
      "\n",
      "Recall: 0.7352941176470589\n",
      "\n",
      "Precision: 0.6756756756756757\n",
      "\n",
      "F1 Score: 0.7042253521126761\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.711864406779661\n",
      "\n",
      "Recall: 0.5588235294117647\n",
      "\n",
      "Precision: 0.9047619047619048\n",
      "\n",
      "F1 Score: 0.6909090909090908\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.6779661016949152\n",
      "\n",
      "Recall: 0.6470588235294118\n",
      "\n",
      "Precision: 0.7586206896551724\n",
      "\n",
      "F1 Score: 0.6984126984126984\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6949152542372882\n",
      "\n",
      "Recall: 0.7352941176470589\n",
      "\n",
      "Precision: 0.7352941176470589\n",
      "\n",
      "F1 Score: 0.735294117647059\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6271186440677966\n",
      "\n",
      "Recall: 0.7941176470588235\n",
      "\n",
      "Precision: 0.6428571428571429\n",
      "\n",
      "F1 Score: 0.7105263157894737\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.7777777777777778\n",
      "\n",
      "F1 Score: 0.7999999999999999\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 0.7941176470588235\n",
      "\n",
      "Precision: 0.7714285714285715\n",
      "\n",
      "F1 Score: 0.782608695652174\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.5932203389830508\n",
      "\n",
      "Recall: 0.5588235294117647\n",
      "\n",
      "Precision: 0.6785714285714286\n",
      "\n",
      "F1 Score: 0.6129032258064516\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7631578947368421\n",
      "\n",
      "F1 Score: 0.8055555555555555\n",
      "\n",
      "[11:26:50] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8108108108108109\n",
      "\n",
      "F1 Score: 0.8450704225352113\n",
      "\n",
      "[11:26:51] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n",
      "-----------------------\n",
      "Top Model:  AdaBoostClassifier(random_state=42)\n",
      "Top F1 Score 0.8648648648648648\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df without dropped columns:\")\n",
    "print(\"not normalized:\")\n",
    "print(\"downsample\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection(X_train_downsample, y_train_downsample, X_test, y_test, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Dropped Columns Normalized Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df without dropped columns:\n",
      "normalized:\n",
      "downsample\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.8235294117647058\n",
      "\n",
      "F1 Score: 0.8235294117647058\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.8055555555555556\n",
      "\n",
      "F1 Score: 0.8285714285714286\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8205128205128205\n",
      "\n",
      "F1 Score: 0.8767123287671232\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.7941176470588235\n",
      "\n",
      "Precision: 0.7941176470588235\n",
      "\n",
      "F1 Score: 0.7941176470588235\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8378378378378378\n",
      "\n",
      "F1 Score: 0.8732394366197184\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.775\n",
      "\n",
      "F1 Score: 0.8378378378378379\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 0.7352941176470589\n",
      "\n",
      "Precision: 0.8064516129032258\n",
      "\n",
      "F1 Score: 0.7692307692307693\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.7941176470588235\n",
      "\n",
      "Precision: 0.8181818181818182\n",
      "\n",
      "F1 Score: 0.8059701492537314\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.8484848484848485\n",
      "\n",
      "F1 Score: 0.8358208955223881\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.8484848484848485\n",
      "\n",
      "F1 Score: 0.8358208955223881\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7288135593220338\n",
      "\n",
      "Recall: 0.7647058823529411\n",
      "\n",
      "Precision: 0.7647058823529411\n",
      "\n",
      "F1 Score: 0.7647058823529412\n",
      "\n",
      "[11:26:54] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8571428571428571\n",
      "\n",
      "F1 Score: 0.8695652173913043\n",
      "\n",
      "[11:26:54] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8115942028985507\n",
      "\n",
      "-----------------------\n",
      "Top Model:  LogisticRegression(random_state=42)\n",
      "Top F1 Score 0.8767123287671232\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df without dropped columns:\")\n",
    "print(\"normalized:\")\n",
    "print(\"downsample\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection_normalize(X_train_downsample, y_train_downsample, X_test, y_test, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropped Columns Not Normalized SMOTE Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns:\n",
      "not normalized:\n",
      "SMOTE\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8205128205128205\n",
      "\n",
      "F1 Score: 0.8767123287671232\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.864406779661017\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.8095238095238095\n",
      "\n",
      "F1 Score: 0.8947368421052632\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.8048780487804879\n",
      "\n",
      "F1 Score: 0.8800000000000001\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8108108108108109\n",
      "\n",
      "F1 Score: 0.8450704225352113\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.711864406779661\n",
      "\n",
      "Recall: 0.6764705882352942\n",
      "\n",
      "Precision: 0.7931034482758621\n",
      "\n",
      "F1 Score: 0.7301587301587301\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.711864406779661\n",
      "\n",
      "Recall: 0.5882352941176471\n",
      "\n",
      "Precision: 0.8695652173913043\n",
      "\n",
      "F1 Score: 0.7017543859649124\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8813559322033898\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.8461538461538461\n",
      "\n",
      "F1 Score: 0.9041095890410958\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8813559322033898\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.8292682926829268\n",
      "\n",
      "F1 Score: 0.9066666666666667\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.8048780487804879\n",
      "\n",
      "F1 Score: 0.8800000000000001\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8157894736842105\n",
      "\n",
      "F1 Score: 0.861111111111111\n",
      "\n",
      "[11:26:56] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8205128205128205\n",
      "\n",
      "F1 Score: 0.8767123287671232\n",
      "\n",
      "[11:26:56] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8157894736842105\n",
      "\n",
      "F1 Score: 0.861111111111111\n",
      "\n",
      "-----------------------\n",
      "Top Model:  HistGradientBoostingClassifier(random_state=42)\n",
      "Top F1 Score 0.9066666666666667\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns:\")\n",
    "print(\"not normalized:\")\n",
    "print(\"SMOTE\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection(X_train_drop_smote_sex, y_train_drop_smote_sex, X_test_drop, y_test_drop, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropped Columns Normalized SMOTE Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns:\n",
      "normalized:\n",
      "SMOTE\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8108108108108109\n",
      "\n",
      "F1 Score: 0.8450704225352113\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.864406779661017\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.8095238095238095\n",
      "\n",
      "F1 Score: 0.8947368421052632\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 0.7941176470588235\n",
      "\n",
      "Precision: 0.7714285714285715\n",
      "\n",
      "F1 Score: 0.782608695652174\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8333333333333334\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 0.7941176470588235\n",
      "\n",
      "Precision: 0.7714285714285715\n",
      "\n",
      "F1 Score: 0.782608695652174\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6779661016949152\n",
      "\n",
      "Recall: 0.5588235294117647\n",
      "\n",
      "Precision: 0.8260869565217391\n",
      "\n",
      "F1 Score: 0.6666666666666667\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.864406779661017\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8421052631578947\n",
      "\n",
      "F1 Score: 0.8888888888888888\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.864406779661017\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8421052631578947\n",
      "\n",
      "F1 Score: 0.8888888888888888\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8205128205128205\n",
      "\n",
      "F1 Score: 0.8767123287671232\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.8048780487804879\n",
      "\n",
      "F1 Score: 0.8800000000000001\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8108108108108109\n",
      "\n",
      "F1 Score: 0.8450704225352113\n",
      "\n",
      "[11:26:58] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8333333333333334\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "[11:26:58] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8157894736842105\n",
      "\n",
      "F1 Score: 0.861111111111111\n",
      "\n",
      "-----------------------\n",
      "Top Model:  SVC(random_state=42)\n",
      "Top F1 Score 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns:\")\n",
    "print(\"normalized:\")\n",
    "print(\"SMOTE\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection_normalize(X_train_drop_smote_sex, y_train_drop_smote_sex, X_test_drop, y_test_drop, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No  Dropped Columns Not Normalized SMOTE Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df without dropped columns:\n",
      "not normalized:\n",
      "SMOTE\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.7777777777777778\n",
      "\n",
      "F1 Score: 0.7999999999999999\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6271186440677966\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.6363636363636364\n",
      "\n",
      "F1 Score: 0.717948717948718\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7631578947368421\n",
      "\n",
      "F1 Score: 0.8055555555555555\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 0.7941176470588235\n",
      "\n",
      "Precision: 0.7714285714285715\n",
      "\n",
      "F1 Score: 0.782608695652174\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.7288135593220338\n",
      "\n",
      "Recall: 0.7352941176470589\n",
      "\n",
      "Precision: 0.78125\n",
      "\n",
      "F1 Score: 0.7575757575757576\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6101694915254238\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.6122448979591837\n",
      "\n",
      "F1 Score: 0.7228915662650602\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.4406779661016949\n",
      "\n",
      "Recall: 0.029411764705882353\n",
      "\n",
      "Precision: 1.0\n",
      "\n",
      "F1 Score: 0.05714285714285715\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8205128205128205\n",
      "\n",
      "F1 Score: 0.8767123287671232\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8108108108108109\n",
      "\n",
      "F1 Score: 0.8450704225352113\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6271186440677966\n",
      "\n",
      "Recall: 0.6470588235294118\n",
      "\n",
      "Precision: 0.6875\n",
      "\n",
      "F1 Score: 0.6666666666666667\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.7777777777777778\n",
      "\n",
      "F1 Score: 0.7999999999999999\n",
      "\n",
      "[11:27:02] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8108108108108109\n",
      "\n",
      "F1 Score: 0.8450704225352113\n",
      "\n",
      "[11:27:02] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7837837837837838\n",
      "\n",
      "F1 Score: 0.8169014084507041\n",
      "\n",
      "-----------------------\n",
      "Top Model:  GradientBoostingClassifier(random_state=42)\n",
      "Top F1 Score 0.8767123287671232\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df without dropped columns:\")\n",
    "print(\"not normalized:\")\n",
    "print(\"SMOTE\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection(X_train_smote_sex, y_train_smote_sex, X_test, y_test, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Dropped Columns Normalized SMOTE Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df without dropped columns:\n",
      "normalized:\n",
      "SMOTE\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7288135593220338\n",
      "\n",
      "Recall: 0.7647058823529411\n",
      "\n",
      "Precision: 0.7647058823529411\n",
      "\n",
      "F1 Score: 0.7647058823529412\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7380952380952381\n",
      "\n",
      "F1 Score: 0.8157894736842106\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7837837837837838\n",
      "\n",
      "F1 Score: 0.8169014084507041\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.6949152542372882\n",
      "\n",
      "Recall: 0.6764705882352942\n",
      "\n",
      "Precision: 0.7666666666666667\n",
      "\n",
      "F1 Score: 0.71875\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.8055555555555556\n",
      "\n",
      "F1 Score: 0.8285714285714286\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.7619047619047619\n",
      "\n",
      "F1 Score: 0.8421052631578947\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6101694915254238\n",
      "\n",
      "Recall: 0.3235294117647059\n",
      "\n",
      "Precision: 1.0\n",
      "\n",
      "F1 Score: 0.48888888888888893\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8333333333333334\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.7777777777777778\n",
      "\n",
      "F1 Score: 0.7999999999999999\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7837837837837838\n",
      "\n",
      "F1 Score: 0.8169014084507041\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8157894736842105\n",
      "\n",
      "F1 Score: 0.861111111111111\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6949152542372882\n",
      "\n",
      "Recall: 0.7058823529411765\n",
      "\n",
      "Precision: 0.75\n",
      "\n",
      "F1 Score: 0.7272727272727272\n",
      "\n",
      "[11:27:06] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.7941176470588235\n",
      "\n",
      "Precision: 0.7941176470588235\n",
      "\n",
      "F1 Score: 0.7941176470588235\n",
      "\n",
      "[11:27:07] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.7777777777777778\n",
      "\n",
      "F1 Score: 0.7999999999999999\n",
      "\n",
      "-----------------------\n",
      "Top Model:  GaussianProcessClassifier(random_state=42)\n",
      "Top F1 Score 0.861111111111111\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df without dropped columns:\")\n",
    "print(\"normalized:\")\n",
    "print(\"SMOTE\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection_normalize(X_train_smote_sex, y_train_smote_sex, X_test, y_test, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropped Columns Not Normalized SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns:\n",
      "not normalized:\n",
      "SMOTE\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.8048780487804879\n",
      "\n",
      "F1 Score: 0.8800000000000001\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.8048780487804879\n",
      "\n",
      "F1 Score: 0.8800000000000001\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7837837837837838\n",
      "\n",
      "F1 Score: 0.8169014084507041\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8108108108108109\n",
      "\n",
      "F1 Score: 0.8450704225352113\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.711864406779661\n",
      "\n",
      "Recall: 0.6176470588235294\n",
      "\n",
      "Precision: 0.84\n",
      "\n",
      "F1 Score: 0.711864406779661\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.8048780487804879\n",
      "\n",
      "F1 Score: 0.8800000000000001\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.864406779661017\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.825\n",
      "\n",
      "F1 Score: 0.8918918918918919\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.7727272727272727\n",
      "\n",
      "F1 Score: 0.8717948717948718\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8205128205128205\n",
      "\n",
      "F1 Score: 0.8767123287671232\n",
      "\n",
      "[11:27:09] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "[11:27:09] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.864406779661017\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.8095238095238095\n",
      "\n",
      "F1 Score: 0.8947368421052632\n",
      "\n",
      "-----------------------\n",
      "Top Model:  XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "Top F1 Score 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns:\")\n",
    "print(\"not normalized:\")\n",
    "print(\"SMOTE\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection(X_train_drop_smote, y_train_drop_smote, X_test_drop, y_test_drop, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropped Columns Normalized SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns:\n",
      "normalized:\n",
      "SMOTE\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8157894736842105\n",
      "\n",
      "F1 Score: 0.861111111111111\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8205128205128205\n",
      "\n",
      "F1 Score: 0.8767123287671232\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7837837837837838\n",
      "\n",
      "F1 Score: 0.8169014084507041\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9705882352941176\n",
      "\n",
      "Precision: 0.8048780487804879\n",
      "\n",
      "F1 Score: 0.8800000000000001\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.7777777777777778\n",
      "\n",
      "F1 Score: 0.7999999999999999\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7837837837837838\n",
      "\n",
      "F1 Score: 0.8169014084507041\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.7941176470588235\n",
      "\n",
      "Precision: 0.84375\n",
      "\n",
      "F1 Score: 0.8181818181818182\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.7777777777777778\n",
      "\n",
      "F1 Score: 0.7999999999999999\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8378378378378378\n",
      "\n",
      "F1 Score: 0.8732394366197184\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8157894736842105\n",
      "\n",
      "F1 Score: 0.861111111111111\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8205128205128205\n",
      "\n",
      "F1 Score: 0.8767123287671232\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8378378378378378\n",
      "\n",
      "F1 Score: 0.8732394366197184\n",
      "\n",
      "[11:27:11] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.8333333333333334\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "[11:27:12] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8157894736842105\n",
      "\n",
      "F1 Score: 0.861111111111111\n",
      "\n",
      "-----------------------\n",
      "Top Model:  MLPClassifier(random_state=42)\n",
      "Top F1 Score 0.8800000000000001\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns:\")\n",
    "print(\"normalized:\")\n",
    "print(\"SMOTE\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection_normalize(X_train_drop_smote, y_train_drop_smote, X_test_drop, y_test_drop, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Dropped Columns Not Normalized SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df without dropped columns:\n",
      "not normalized:\n",
      "SMOTE\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6610169491525424\n",
      "\n",
      "Recall: 0.7352941176470589\n",
      "\n",
      "Precision: 0.6944444444444444\n",
      "\n",
      "F1 Score: 0.7142857142857144\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7894736842105263\n",
      "\n",
      "F1 Score: 0.8333333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7837837837837838\n",
      "\n",
      "F1 Score: 0.8169014084507041\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.6271186440677966\n",
      "\n",
      "Recall: 0.5882352941176471\n",
      "\n",
      "Precision: 0.7142857142857143\n",
      "\n",
      "F1 Score: 0.6451612903225806\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6440677966101694\n",
      "\n",
      "Recall: 0.5\n",
      "\n",
      "Precision: 0.8095238095238095\n",
      "\n",
      "F1 Score: 0.6181818181818182\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.4745762711864407\n",
      "\n",
      "Recall: 0.08823529411764706\n",
      "\n",
      "Precision: 1.0\n",
      "\n",
      "F1 Score: 0.1621621621621622\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7560975609756098\n",
      "\n",
      "F1 Score: 0.8266666666666665\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.5932203389830508\n",
      "\n",
      "Recall: 0.5588235294117647\n",
      "\n",
      "Precision: 0.6785714285714286\n",
      "\n",
      "F1 Score: 0.6129032258064516\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6949152542372882\n",
      "\n",
      "Recall: 0.7058823529411765\n",
      "\n",
      "Precision: 0.75\n",
      "\n",
      "F1 Score: 0.7272727272727272\n",
      "\n",
      "[11:27:15] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7380952380952381\n",
      "\n",
      "F1 Score: 0.8157894736842106\n",
      "\n",
      "[11:27:15] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7560975609756098\n",
      "\n",
      "F1 Score: 0.8266666666666665\n",
      "\n",
      "-----------------------\n",
      "Top Model:  GaussianNB()\n",
      "Top F1 Score 0.8648648648648648\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df without dropped columns:\")\n",
    "print(\"not normalized:\")\n",
    "print(\"SMOTE\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection(X_train_smote, y_train_smote, X_test, y_test, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Dropped Columns Normalized SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df without dropped columns:\n",
      "normalized:\n",
      "SMOTE\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.7837837837837838\n",
      "\n",
      "F1 Score: 0.8169014084507041\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.7619047619047619\n",
      "\n",
      "F1 Score: 0.8421052631578947\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.7948717948717948\n",
      "\n",
      "F1 Score: 0.8493150684931507\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7796610169491526\n",
      "\n",
      "Recall: 0.8823529411764706\n",
      "\n",
      "Precision: 0.7692307692307693\n",
      "\n",
      "F1 Score: 0.8219178082191781\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Recall: 0.7941176470588235\n",
      "\n",
      "Precision: 0.7941176470588235\n",
      "\n",
      "F1 Score: 0.7941176470588235\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9117647058823529\n",
      "\n",
      "Precision: 0.8157894736842105\n",
      "\n",
      "F1 Score: 0.861111111111111\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7457627118644068\n",
      "\n",
      "Recall: 0.7647058823529411\n",
      "\n",
      "Precision: 0.7878787878787878\n",
      "\n",
      "F1 Score: 0.7761194029850745\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.7391304347826086\n",
      "\n",
      "F1 Score: 0.85\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.8055555555555556\n",
      "\n",
      "F1 Score: 0.8285714285714286\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8205128205128205\n",
      "\n",
      "F1 Score: 0.8767123287671232\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Recall: 0.8529411764705882\n",
      "\n",
      "Precision: 0.8055555555555556\n",
      "\n",
      "F1 Score: 0.8285714285714286\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7288135593220338\n",
      "\n",
      "Recall: 0.7647058823529411\n",
      "\n",
      "Precision: 0.7647058823529411\n",
      "\n",
      "F1 Score: 0.7647058823529412\n",
      "\n",
      "[11:27:17] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Recall: 0.9411764705882353\n",
      "\n",
      "Precision: 0.7804878048780488\n",
      "\n",
      "F1 Score: 0.8533333333333334\n",
      "\n",
      "[11:27:18] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7288135593220338\n",
      "\n",
      "Recall: 0.8235294117647058\n",
      "\n",
      "Precision: 0.7368421052631579\n",
      "\n",
      "F1 Score: 0.7777777777777778\n",
      "\n",
      "-----------------------\n",
      "Top Model:  AdaBoostClassifier(random_state=42)\n",
      "Top F1 Score 0.8767123287671232\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df without dropped columns:\")\n",
    "print(\"normalized:\")\n",
    "print(\"SMOTE\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection_normalize(X_train_smote, y_train_smote, X_test, y_test, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "\n",
    "# Number of trees to be used\n",
    "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
    "\n",
    "# Minimum number of instaces needed in each node\n",
    "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
    "\n",
    "# Tree construction algorithm used in XGBoost\n",
    "xgb_tree_method = ['auto', 'exact', 'approx', 'hist', 'gpu_hist']\n",
    "\n",
    "# Learning rate\n",
    "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
    "\n",
    "# Minimum loss reduction required to make further partition\n",
    "xgb_gamma = [int(x) for x in np.linspace(0, 0.5, 6)]\n",
    "\n",
    "# Learning objective used\n",
    "xgb_objective = ['reg:squarederror', 'reg:squaredlogerror']\n",
    "\n",
    "# Create the grid\n",
    "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
    "            'max_depth': xgb_max_depth,\n",
    "            'min_child_weight': xgb_min_child_weight,\n",
    "            'tree_method': xgb_tree_method,\n",
    "            'eta': xgb_eta,\n",
    "            'gamma': xgb_gamma,\n",
    "            'objective': xgb_objective}\n",
    "# Define grid search\n",
    "grid = RandomizedSearchCV(\n",
    "  XGBRFClassifier(use_label_encoder= False),\n",
    "  param_distributions=xgb_grid,\n",
    "  scoring=\"f1\",\n",
    "  n_iter = 200,\n",
    "  random_state = 42,\n",
    "  verbose=0,\n",
    "  n_jobs=-1,\n",
    ")\n",
    "\n",
    "\n",
    "#best_model = grid.fit(X_train_drop, y_train_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_params = best_model.best_params_\n",
    "#best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:51:18] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Test Set Metrics\n",
      "-----------------\n",
      "Accuracy: 0.8813559322033898\n",
      "Recall: 1.0\n",
      "Precision: 0.8292682926829268\n",
      "F1 Score: 0.9066666666666667\n"
     ]
    }
   ],
   "source": [
    " from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "#clf = XGBRFClassifier( use_label_encoder=False,tree_method = \"approx\", objective = \"reg:squarederror\",\n",
    "#                      eta = 0.3, gamma=0,  max_depth = 4, n_estimators = 800,min_child_weight = 10, random_state = 42)\n",
    "\n",
    "clf = XGBRFClassifier(random_state = 42)\n",
    "#early stopping rounds = 5\n",
    "clf.fit(X_train_drop, y_train_drop)\n",
    "\n",
    "predictions = clf.predict(X_test_drop)\n",
    "actual = y_test_drop\n",
    "\n",
    "print(\"Test Set Metrics\")\n",
    "print(\"-----------------\")\n",
    "print(\"Accuracy: \" + str(clf.score(X_test_drop, y_test_drop)))\n",
    "print(\"Recall: \" + str(recall_score(actual, predictions)))\n",
    "print(\"Precision: \" + str(precision_score(actual, predictions)))\n",
    "print(\"F1 Score: \" + str(f1_score(actual, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(clf, X_test_drop, y_test_drop)  \n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
