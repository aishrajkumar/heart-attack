{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trtbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalachh</th>\n",
       "      <th>exng</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slp</th>\n",
       "      <th>caa</th>\n",
       "      <th>thall</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n",
       "0   63    1   3     145   233    1        0       150     0      2.3    0   \n",
       "1   37    1   2     130   250    0        1       187     0      3.5    0   \n",
       "2   41    0   1     130   204    0        0       172     0      1.4    2   \n",
       "3   56    1   1     120   236    0        1       178     0      0.8    2   \n",
       "4   57    0   0     120   354    0        1       163     1      0.6    2   \n",
       "\n",
       "   caa  thall  output  \n",
       "0    0      1       1  \n",
       "1    0      2       1  \n",
       "2    0      2       1  \n",
       "3    0      2       1  \n",
       "4    0      2       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in dataframe\n",
    "df = pd.read_csv('./heart.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Outlier Factor: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "LOF = LocalOutlierFactor(n_neighbors = 20, contamination = \"auto\")\n",
    "\n",
    "lof_outliers = LOF.fit_predict(df)\n",
    "print(\"Local Outlier Factor: \" + str(lof_outliers[np.where(lof_outliers == -1)].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_list = np.where(lof_outliers == -1)[0].tolist()\n",
    "df = df.drop(outlier_list)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop = df.drop([\"trtbps\", \"thalachh\", \"age\",\"chol\",\"thall\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = MinMaxScaler()\n",
    "\n",
    "X_drop = df_drop.drop(\"output\", axis = 1)\n",
    "y_drop = df_drop[\"output\"]\n",
    "\n",
    "X = df.drop(\"output\", axis = 1)\n",
    "y = df[\"output\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_drop, X_test_drop, y_train_drop, y_test_drop = train_test_split(X_drop, y_drop,test_size = 0.3, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#col = None\n",
    "#oversample = SMOTE(k_neighbors=4)\n",
    "#for col in df.drop(\"oldpeak\", axis = 1).columns.values:\n",
    "#    print(col)\n",
    "#    X = df\n",
    "#    y = df[col]\n",
    "#    df,y = oversample.fit_resample(X,y)\n",
    "#\n",
    "#df.hist(figsize=(20,10))\n",
    "#print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier, HistGradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.metrics import recall_score,precision_score, f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from xgboost import XGBClassifier,XGBRFClassifier\n",
    "\n",
    "\n",
    "def model_selection(X_train, y_train,X_test, y_test, estimator):\n",
    "    \"\"\"\n",
    "    Test various estimators.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    model = estimator\n",
    "\n",
    "    # Instantiate the classification model and visualizer\n",
    "    model.fit(X_train, y_train)  \n",
    "    \n",
    "    expected  = y_test\n",
    "    predicted = model.predict(X_test)\n",
    "    \n",
    "\n",
    "    # Compute and return the F1 score (the harmonic mean of precision and recall)\n",
    "    return [model.score(X_test,y_test), recall_score(expected, predicted), precision_score(expected, predicted), f1_score(expected, predicted)]\n",
    "\n",
    "def model_selection_normalize(X_train, y_train,X_test, y_test, estimator):\n",
    "    \"\"\"\n",
    "    Test various estimators.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    model = estimator\n",
    "\n",
    "    # Instantiate the classification model and visualizer\n",
    "    model.fit(normalize.fit_transform(X_train), y_train)  \n",
    "    \n",
    "    expected  = y_test\n",
    "    predicted = model.predict(normalize.fit_transform(X_test))\n",
    "    \n",
    "\n",
    "    # Compute and return the F1 score (the harmonic mean of precision and recall)\n",
    "    return [model.score(normalize.fit_transform(X_test),y_test), recall_score(expected, predicted), precision_score(expected, predicted), f1_score(expected, predicted)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_lst = [\n",
    "                 RandomForestClassifier(random_state = 42),\n",
    "                 SVC(random_state = 42),\n",
    "                 LogisticRegression(random_state = 42),\n",
    "                 MLPClassifier(random_state = 42),\n",
    "                 GaussianNB(),\n",
    "                 KNeighborsClassifier(),\n",
    "                 SGDClassifier(random_state = 42),\n",
    "                 PassiveAggressiveClassifier(random_state = 42),\n",
    "                 GradientBoostingClassifier(random_state = 42),\n",
    "                 AdaBoostClassifier(random_state = 42),\n",
    "                 HistGradientBoostingClassifier(random_state = 42),\n",
    "                 GaussianProcessClassifier(random_state = 42),\n",
    "                 BaggingClassifier(random_state = 42),\n",
    "                 XGBClassifier(random_state = 42,use_label_encoder=False),\n",
    "                 XGBRFClassifier(random_state = 42, use_label_encoder=False)\n",
    "                \n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF with all columns not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns\n",
      "not normalized\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8727272727272727\n",
      "\n",
      "F1 Score: 0.8727272727272727\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7159090909090909\n",
      "\n",
      "Recall: 0.8363636363636363\n",
      "\n",
      "Precision: 0.7419354838709677\n",
      "\n",
      "F1 Score: 0.7863247863247863\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.875\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8666666666666667\n",
      "\n",
      "F1 Score: 0.9043478260869566\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.75\n",
      "\n",
      "Recall: 0.7636363636363637\n",
      "\n",
      "Precision: 0.8235294117647058\n",
      "\n",
      "F1 Score: 0.7924528301886793\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.9272727272727272\n",
      "\n",
      "Precision: 0.85\n",
      "\n",
      "F1 Score: 0.8869565217391303\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.625\n",
      "\n",
      "Recall: 0.6363636363636364\n",
      "\n",
      "Precision: 0.7291666666666666\n",
      "\n",
      "F1 Score: 0.6796116504854369\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6704545454545454\n",
      "\n",
      "Recall: 0.9636363636363636\n",
      "\n",
      "Precision: 0.6625\n",
      "\n",
      "F1 Score: 0.7851851851851853\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6704545454545454\n",
      "\n",
      "Recall: 0.9272727272727272\n",
      "\n",
      "Precision: 0.6710526315789473\n",
      "\n",
      "F1 Score: 0.7786259541984732\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7954545454545454\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8245614035087719\n",
      "\n",
      "F1 Score: 0.8392857142857144\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8421052631578947\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8068181818181818\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8392857142857143\n",
      "\n",
      "F1 Score: 0.8468468468468467\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6022727272727273\n",
      "\n",
      "Recall: 0.6\n",
      "\n",
      "Precision: 0.717391304347826\n",
      "\n",
      "F1 Score: 0.6534653465346534\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7840909090909091\n",
      "\n",
      "Recall: 0.8\n",
      "\n",
      "Precision: 0.8461538461538461\n",
      "\n",
      "F1 Score: 0.8224299065420562\n",
      "\n",
      "[19:44:24] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7840909090909091\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8103448275862069\n",
      "\n",
      "F1 Score: 0.831858407079646\n",
      "\n",
      "[19:44:24] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8068181818181818\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8166666666666667\n",
      "\n",
      "F1 Score: 0.8521739130434782\n",
      "\n",
      "-----------------------\n",
      "Top Model:  LogisticRegression(random_state=42)\n",
      "Top F1 Score:  0.9043478260869566\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns\")\n",
    "print(\"not normalized\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection(X_train, y_train, X_test, y_test, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score: \", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF with all columns normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns\n",
      "normalized\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8596491228070176\n",
      "\n",
      "F1 Score: 0.875\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7840909090909091\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8347826086956521\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.875\n",
      "\n",
      "Recall: 0.9636363636363636\n",
      "\n",
      "Precision: 0.8548387096774194\n",
      "\n",
      "F1 Score: 0.905982905982906\n",
      "\n",
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.847457627118644\n",
      "\n",
      "F1 Score: 0.8771929824561402\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8571428571428571\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8596491228070176\n",
      "\n",
      "F1 Score: 0.875\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9272727272727272\n",
      "\n",
      "Precision: 0.864406779661017\n",
      "\n",
      "F1 Score: 0.8947368421052632\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.7857142857142857\n",
      "\n",
      "F1 Score: 0.88\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7954545454545454\n",
      "\n",
      "Recall: 0.8181818181818182\n",
      "\n",
      "Precision: 0.8490566037735849\n",
      "\n",
      "F1 Score: 0.8333333333333334\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8703703703703703\n",
      "\n",
      "F1 Score: 0.8623853211009175\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7954545454545454\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8245614035087719\n",
      "\n",
      "F1 Score: 0.8392857142857144\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8524590163934426\n",
      "\n",
      "F1 Score: 0.8965517241379309\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8068181818181818\n",
      "\n",
      "Recall: 0.7636363636363637\n",
      "\n",
      "Precision: 0.9130434782608695\n",
      "\n",
      "F1 Score: 0.8316831683168316\n",
      "\n",
      "[19:44:25] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8068181818181818\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8275862068965517\n",
      "\n",
      "F1 Score: 0.8495575221238938\n",
      "\n",
      "[19:44:26] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7954545454545454\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8135593220338984\n",
      "\n",
      "F1 Score: 0.8421052631578948\n",
      "\n",
      "-----------------------\n",
      "Top Model:  LogisticRegression(random_state=42)\n",
      "Top F1 Score:  0.905982905982906\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns\")\n",
    "print(\"normalized\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection_normalize(X_train, y_train, X_test, y_test, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score: \", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF with columns removed not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns:\n",
      "not normalized:\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8448275862068966\n",
      "\n",
      "F1 Score: 0.8672566371681416\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8977272727272727\n",
      "\n",
      "Recall: 0.9818181818181818\n",
      "\n",
      "Precision: 0.8709677419354839\n",
      "\n",
      "F1 Score: 0.923076923076923\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.9272727272727272\n",
      "\n",
      "Precision: 0.85\n",
      "\n",
      "F1 Score: 0.8869565217391303\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8863636363636364\n",
      "\n",
      "Recall: 0.9818181818181818\n",
      "\n",
      "Precision: 0.8571428571428571\n",
      "\n",
      "F1 Score: 0.9152542372881356\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8448275862068966\n",
      "\n",
      "F1 Score: 0.8672566371681416\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8363636363636363\n",
      "\n",
      "Precision: 0.8679245283018868\n",
      "\n",
      "F1 Score: 0.8518518518518519\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.5909090909090909\n",
      "\n",
      "Recall: 0.38181818181818183\n",
      "\n",
      "Precision: 0.9130434782608695\n",
      "\n",
      "F1 Score: 0.5384615384615384\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8387096774193549\n",
      "\n",
      "F1 Score: 0.8888888888888888\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.8620689655172413\n",
      "\n",
      "F1 Score: 0.8849557522123893\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9636363636363636\n",
      "\n",
      "Precision: 0.8412698412698413\n",
      "\n",
      "F1 Score: 0.8983050847457628\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8863636363636364\n",
      "\n",
      "Recall: 0.9636363636363636\n",
      "\n",
      "Precision: 0.8688524590163934\n",
      "\n",
      "F1 Score: 0.9137931034482758\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.875\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8666666666666667\n",
      "\n",
      "F1 Score: 0.9043478260869566\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8068181818181818\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8392857142857143\n",
      "\n",
      "F1 Score: 0.8468468468468467\n",
      "\n",
      "[19:44:27] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8545454545454545\n",
      "\n",
      "F1 Score: 0.8545454545454545\n",
      "\n",
      "[19:44:27] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9272727272727272\n",
      "\n",
      "Precision: 0.864406779661017\n",
      "\n",
      "F1 Score: 0.8947368421052632\n",
      "\n",
      "-----------------------\n",
      "Top Model:  SVC(random_state=42)\n",
      "Top F1 Score 0.923076923076923\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns:\")\n",
    "print(\"not normalized:\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection(X_train_drop, y_train_drop, X_test_drop, y_test_drop, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF with columns removed normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns:\n",
      "normalized:\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.875\n",
      "\n",
      "F1 Score: 0.8828828828828829\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.8620689655172413\n",
      "\n",
      "F1 Score: 0.8849557522123893\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8448275862068966\n",
      "\n",
      "F1 Score: 0.8672566371681416\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8448275862068966\n",
      "\n",
      "F1 Score: 0.8672566371681416\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8421052631578947\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.7954545454545454\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8245614035087719\n",
      "\n",
      "F1 Score: 0.8392857142857144\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8068181818181818\n",
      "\n",
      "Recall: 0.8181818181818182\n",
      "\n",
      "Precision: 0.8653846153846154\n",
      "\n",
      "F1 Score: 0.8411214953271028\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.847457627118644\n",
      "\n",
      "F1 Score: 0.8771929824561402\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8596491228070176\n",
      "\n",
      "F1 Score: 0.875\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.875\n",
      "\n",
      "Recall: 0.9636363636363636\n",
      "\n",
      "Precision: 0.8548387096774194\n",
      "\n",
      "F1 Score: 0.905982905982906\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8977272727272727\n",
      "\n",
      "Recall: 0.9818181818181818\n",
      "\n",
      "Precision: 0.8709677419354839\n",
      "\n",
      "F1 Score: 0.923076923076923\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8524590163934426\n",
      "\n",
      "F1 Score: 0.8965517241379309\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7954545454545454\n",
      "\n",
      "Recall: 0.8\n",
      "\n",
      "Precision: 0.8627450980392157\n",
      "\n",
      "F1 Score: 0.8301886792452831\n",
      "\n",
      "[19:44:28] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8545454545454545\n",
      "\n",
      "F1 Score: 0.8545454545454545\n",
      "\n",
      "[19:44:28] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.8620689655172413\n",
      "\n",
      "F1 Score: 0.8849557522123893\n",
      "\n",
      "-----------------------\n",
      "Top Model:  HistGradientBoostingClassifier(random_state=42)\n",
      "Top F1 Score 0.923076923076923\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns:\")\n",
    "print(\"normalized:\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection_normalize(X_train_drop, y_train_drop, X_test_drop, y_test_drop, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "X_resample_drop = pd.concat([X_train_drop, y_train_drop], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "female_drop = X_resample_drop[X_resample_drop.output==0]\n",
    "male_drop = X_resample_drop[X_resample_drop.output==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resample = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "female= X_resample[X_resample.output==0]\n",
    "male= X_resample[X_resample.output==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample minority\n",
    "gender_upsample_drop = resample(female_drop,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(male_drop), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsample_drop = pd.concat([male_drop, gender_upsample_drop])\n",
    "\n",
    "X_train_drop_upsample = upsample_drop.drop(\"output\", axis = 1)\n",
    "y_train_drop_upsample = upsample_drop[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample minority\n",
    "gender_downsample_drop = resample(male_drop,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(female_drop), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "downsample_drop = pd.concat([gender_downsample_drop, female_drop])\n",
    "\n",
    "X_train_drop_downsample = downsample_drop.drop(\"output\", axis = 1)\n",
    "y_train_drop_downsample = downsample_drop[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample minority\n",
    "gender_upsample = resample(female,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(male), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsample = pd.concat([male, gender_upsample])\n",
    "X_train_upsample = upsample.drop(\"output\", axis = 1)\n",
    "y_train_upsample = upsample[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample majority\n",
    "gender_downsample = resample(male,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(female), # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "# combine minority and downsampled majority\n",
    "downsample = pd.concat([gender_downsample, female])\n",
    "X_train_downsample = downsample.drop(\"output\", axis = 1)\n",
    "y_train_downsample = downsample[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#sm = SMOTE(random_state=42)\n",
    "#X_smote, sex_smote = sm.fit_resample(X_resample.drop(\"sex\", axis = 1), X_resample[\"sex\"])\n",
    "#X_smote[\"sex\"] = sex_smote.values\n",
    "#smote = X_smote\n",
    "\n",
    "#X_train_smote = smote.drop(\"output\", axis = 1)\n",
    "#smote_cols = X_train_smote.columns.tolist()\n",
    "#smote_cols = smote_cols[-1:]  + smote_cols[:-1]\n",
    "#X_train_smote = X_train_smote[smote_cols]\n",
    "#y_train_smote = smote[\"output\"]\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_resample.drop(\"output\", axis = 1), X_resample[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smote.sex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_smote_drop, sex_smote_drop = sm.fit_resample(X_resample_drop.drop(\"sex\", axis = 1), X_resample_drop[\"sex\"])\n",
    "#X_smote_drop[\"sex\"] = sex_smote_drop.values\n",
    "#smote_drop = X_smote_drop\n",
    "\n",
    "#X_train_drop_smote = smote_drop.drop(\"output\", axis = 1)\n",
    "#smote_drop_cols = X_train_drop_smote.columns.tolist()\n",
    "#smote_drop_cols = smote_drop_cols[-1:]  + smote_drop_cols[:-1]\n",
    "#X_train_drop_smote = X_train_drop_smote[smote_drop_cols]\n",
    "#y_train_drop_smote = smote_drop[\"output\"]\n",
    "\n",
    "X_train_drop_smote, y_train_drop_smote = sm.fit_resample(X_resample_drop.drop(\"output\", axis = 1), X_resample_drop[\"output\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropped Columns Not Normalized Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns:\n",
      "not normalized:\n",
      "upsample\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7840909090909091\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8347826086956521\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.875\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8666666666666667\n",
      "\n",
      "F1 Score: 0.9043478260869566\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.847457627118644\n",
      "\n",
      "F1 Score: 0.8771929824561402\n",
      "\n",
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.875\n",
      "\n",
      "Recall: 0.9818181818181818\n",
      "\n",
      "Precision: 0.84375\n",
      "\n",
      "F1 Score: 0.9075630252100839\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8421052631578947\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.75\n",
      "\n",
      "Recall: 0.7272727272727273\n",
      "\n",
      "Precision: 0.851063829787234\n",
      "\n",
      "F1 Score: 0.7843137254901961\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6818181818181818\n",
      "\n",
      "Recall: 0.6727272727272727\n",
      "\n",
      "Precision: 0.7872340425531915\n",
      "\n",
      "F1 Score: 0.7254901960784313\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.7971014492753623\n",
      "\n",
      "F1 Score: 0.8870967741935484\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7954545454545454\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8135593220338984\n",
      "\n",
      "F1 Score: 0.8421052631578948\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8068181818181818\n",
      "\n",
      "Recall: 0.9272727272727272\n",
      "\n",
      "Precision: 0.796875\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.9636363636363636\n",
      "\n",
      "Precision: 0.803030303030303\n",
      "\n",
      "F1 Score: 0.8760330578512396\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9272727272727272\n",
      "\n",
      "Precision: 0.864406779661017\n",
      "\n",
      "F1 Score: 0.8947368421052632\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7727272727272727\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.7966101694915254\n",
      "\n",
      "F1 Score: 0.8245614035087718\n",
      "\n",
      "[19:44:29] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7840909090909091\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.7903225806451613\n",
      "\n",
      "F1 Score: 0.8376068376068375\n",
      "\n",
      "[19:44:29] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8305084745762712\n",
      "\n",
      "F1 Score: 0.8596491228070176\n",
      "\n",
      "-----------------------\n",
      "Top Model:  MLPClassifier(random_state=42)\n",
      "Top F1 Score 0.9075630252100839\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns:\")\n",
    "print(\"not normalized:\")\n",
    "print(\"upsample\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection(X_train_drop_upsample, y_train_drop_upsample, X_test_drop, y_test_drop, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropped Columns Normalized Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns:\n",
      "normalized:\n",
      "upsample\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7727272727272727\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.7966101694915254\n",
      "\n",
      "F1 Score: 0.8245614035087718\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8421052631578947\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8421052631578947\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9636363636363636\n",
      "\n",
      "Precision: 0.8412698412698413\n",
      "\n",
      "F1 Score: 0.8983050847457628\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8448275862068966\n",
      "\n",
      "F1 Score: 0.8672566371681416\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.7840909090909091\n",
      "\n",
      "Recall: 0.8363636363636363\n",
      "\n",
      "Precision: 0.8214285714285714\n",
      "\n",
      "F1 Score: 0.8288288288288289\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8125\n",
      "\n",
      "F1 Score: 0.8739495798319329\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6704545454545454\n",
      "\n",
      "Recall: 0.5454545454545454\n",
      "\n",
      "Precision: 0.8823529411764706\n",
      "\n",
      "F1 Score: 0.6741573033707865\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7613636363636364\n",
      "\n",
      "Recall: 0.8\n",
      "\n",
      "Precision: 0.8148148148148148\n",
      "\n",
      "F1 Score: 0.8073394495412846\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7954545454545454\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.7936507936507936\n",
      "\n",
      "F1 Score: 0.847457627118644\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8305084745762712\n",
      "\n",
      "F1 Score: 0.8596491228070176\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8524590163934426\n",
      "\n",
      "F1 Score: 0.8965517241379309\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7840909090909091\n",
      "\n",
      "Recall: 0.8363636363636363\n",
      "\n",
      "Precision: 0.8214285714285714\n",
      "\n",
      "F1 Score: 0.8288288288288289\n",
      "\n",
      "[19:44:30] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7840909090909091\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8347826086956521\n",
      "\n",
      "[19:44:30] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8421052631578947\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "-----------------------\n",
      "Top Model:  MLPClassifier(random_state=42)\n",
      "Top F1 Score 0.8983050847457628\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns:\")\n",
    "print(\"normalized:\")\n",
    "print(\"upsample\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection_normalize(X_train_drop_upsample, y_train_drop_upsample, X_test_drop, y_test_drop, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Dropped Columns Not Normalized Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df without dropped columns:\n",
      "not normalized:\n",
      "upsample\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.819672131147541\n",
      "\n",
      "F1 Score: 0.8620689655172413\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6931818181818182\n",
      "\n",
      "Recall: 0.7818181818181819\n",
      "\n",
      "Precision: 0.7413793103448276\n",
      "\n",
      "F1 Score: 0.7610619469026548\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.875\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8666666666666667\n",
      "\n",
      "F1 Score: 0.9043478260869566\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7954545454545454\n",
      "\n",
      "Recall: 0.8\n",
      "\n",
      "Precision: 0.8627450980392157\n",
      "\n",
      "F1 Score: 0.8301886792452831\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.9272727272727272\n",
      "\n",
      "Precision: 0.85\n",
      "\n",
      "F1 Score: 0.8869565217391303\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.625\n",
      "\n",
      "Recall: 0.6363636363636364\n",
      "\n",
      "Precision: 0.7291666666666666\n",
      "\n",
      "F1 Score: 0.6796116504854369\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.5909090909090909\n",
      "\n",
      "Recall: 0.38181818181818183\n",
      "\n",
      "Precision: 0.9130434782608695\n",
      "\n",
      "F1 Score: 0.5384615384615384\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6477272727272727\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.6395348837209303\n",
      "\n",
      "F1 Score: 0.7801418439716312\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7727272727272727\n",
      "\n",
      "Recall: 0.8363636363636363\n",
      "\n",
      "Precision: 0.8070175438596491\n",
      "\n",
      "F1 Score: 0.8214285714285714\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8448275862068966\n",
      "\n",
      "F1 Score: 0.8672566371681416\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7840909090909091\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8\n",
      "\n",
      "F1 Score: 0.8347826086956521\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.5909090909090909\n",
      "\n",
      "Recall: 0.6363636363636364\n",
      "\n",
      "Precision: 0.6862745098039216\n",
      "\n",
      "F1 Score: 0.660377358490566\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7954545454545454\n",
      "\n",
      "Recall: 0.8363636363636363\n",
      "\n",
      "Precision: 0.8363636363636363\n",
      "\n",
      "F1 Score: 0.8363636363636363\n",
      "\n",
      "[19:44:31] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8068181818181818\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8166666666666667\n",
      "\n",
      "F1 Score: 0.8521739130434782\n",
      "\n",
      "[19:44:31] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7954545454545454\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8032786885245902\n",
      "\n",
      "F1 Score: 0.8448275862068965\n",
      "\n",
      "-----------------------\n",
      "Top Model:  LogisticRegression(random_state=42)\n",
      "Top F1 Score 0.9043478260869566\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df without dropped columns:\")\n",
    "print(\"not normalized:\")\n",
    "print(\"upsample\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection(X_train_upsample, y_train_upsample, X_test, y_test, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Dropped Columns Normalized Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df without dropped columns:\n",
      "normalized:\n",
      "upsample\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.819672131147541\n",
      "\n",
      "F1 Score: 0.8620689655172413\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8068181818181818\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8166666666666667\n",
      "\n",
      "F1 Score: 0.8521739130434782\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9636363636363636\n",
      "\n",
      "Precision: 0.8412698412698413\n",
      "\n",
      "F1 Score: 0.8983050847457628\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.9272727272727272\n",
      "\n",
      "Precision: 0.8360655737704918\n",
      "\n",
      "F1 Score: 0.8793103448275862\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.9272727272727272\n",
      "\n",
      "Precision: 0.85\n",
      "\n",
      "F1 Score: 0.8869565217391303\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.8068181818181818\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8275862068965517\n",
      "\n",
      "F1 Score: 0.8495575221238938\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9636363636363636\n",
      "\n",
      "Precision: 0.8412698412698413\n",
      "\n",
      "F1 Score: 0.8983050847457628\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8545454545454545\n",
      "\n",
      "F1 Score: 0.8545454545454545\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8305084745762712\n",
      "\n",
      "F1 Score: 0.8596491228070176\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.847457627118644\n",
      "\n",
      "F1 Score: 0.8771929824561402\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8068181818181818\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8166666666666667\n",
      "\n",
      "F1 Score: 0.8521739130434782\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.9272727272727272\n",
      "\n",
      "Precision: 0.85\n",
      "\n",
      "F1 Score: 0.8869565217391303\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.875\n",
      "\n",
      "F1 Score: 0.8828828828828829\n",
      "\n",
      "[19:44:33] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7954545454545454\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8032786885245902\n",
      "\n",
      "F1 Score: 0.8448275862068965\n",
      "\n",
      "[19:44:33] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7954545454545454\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8032786885245902\n",
      "\n",
      "F1 Score: 0.8448275862068965\n",
      "\n",
      "-----------------------\n",
      "Top Model:  LogisticRegression(random_state=42)\n",
      "Top F1 Score 0.8983050847457628\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df without dropped columns:\")\n",
    "print(\"normalized:\")\n",
    "print(\"upsample\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection_normalize(X_train_upsample, y_train_upsample, X_test, y_test, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropped Columns Not Normalized Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns:\n",
      "not normalized:\n",
      "downsample\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7613636363636364\n",
      "\n",
      "Recall: 0.7636363636363637\n",
      "\n",
      "Precision: 0.84\n",
      "\n",
      "F1 Score: 0.8000000000000002\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8727272727272727\n",
      "\n",
      "F1 Score: 0.8727272727272727\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8421052631578947\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.8620689655172413\n",
      "\n",
      "F1 Score: 0.8849557522123893\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.7954545454545454\n",
      "\n",
      "Recall: 0.8363636363636363\n",
      "\n",
      "Precision: 0.8363636363636363\n",
      "\n",
      "F1 Score: 0.8363636363636363\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8181818181818182\n",
      "\n",
      "Precision: 0.8823529411764706\n",
      "\n",
      "F1 Score: 0.8490566037735848\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.8208955223880597\n",
      "\n",
      "F1 Score: 0.9016393442622952\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.875\n",
      "\n",
      "F1 Score: 0.8828828828828829\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7386363636363636\n",
      "\n",
      "Recall: 0.7636363636363637\n",
      "\n",
      "Precision: 0.8076923076923077\n",
      "\n",
      "F1 Score: 0.7850467289719626\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8068181818181818\n",
      "\n",
      "Recall: 0.8363636363636363\n",
      "\n",
      "Precision: 0.8518518518518519\n",
      "\n",
      "F1 Score: 0.8440366972477065\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7954545454545454\n",
      "\n",
      "Recall: 0.8181818181818182\n",
      "\n",
      "Precision: 0.8490566037735849\n",
      "\n",
      "F1 Score: 0.8333333333333334\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8571428571428571\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6931818181818182\n",
      "\n",
      "Recall: 0.6181818181818182\n",
      "\n",
      "Precision: 0.85\n",
      "\n",
      "F1 Score: 0.7157894736842104\n",
      "\n",
      "[19:44:35] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7613636363636364\n",
      "\n",
      "Recall: 0.7818181818181819\n",
      "\n",
      "Precision: 0.8269230769230769\n",
      "\n",
      "F1 Score: 0.8037383177570092\n",
      "\n",
      "[19:44:36] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7613636363636364\n",
      "\n",
      "Recall: 0.7454545454545455\n",
      "\n",
      "Precision: 0.8541666666666666\n",
      "\n",
      "F1 Score: 0.7961165048543689\n",
      "\n",
      "-----------------------\n",
      "Top Model:  SGDClassifier(random_state=42)\n",
      "Top F1 Score 0.9016393442622952\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns:\")\n",
    "print(\"not normalized:\")\n",
    "print(\"downsample\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection(X_train_drop_downsample, y_train_drop_downsample, X_test_drop, y_test_drop, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropped Columns Normalized Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns:\n",
      "normalized:\n",
      "downsample\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7727272727272727\n",
      "\n",
      "Recall: 0.7636363636363637\n",
      "\n",
      "Precision: 0.8571428571428571\n",
      "\n",
      "F1 Score: 0.8076923076923076\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.75\n",
      "\n",
      "Recall: 0.7454545454545455\n",
      "\n",
      "Precision: 0.8367346938775511\n",
      "\n",
      "F1 Score: 0.7884615384615385\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8448275862068966\n",
      "\n",
      "F1 Score: 0.8672566371681416\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8448275862068966\n",
      "\n",
      "F1 Score: 0.8672566371681416\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.7840909090909091\n",
      "\n",
      "Recall: 0.8181818181818182\n",
      "\n",
      "Precision: 0.8333333333333334\n",
      "\n",
      "F1 Score: 0.8256880733944955\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.75\n",
      "\n",
      "Recall: 0.7636363636363637\n",
      "\n",
      "Precision: 0.8235294117647058\n",
      "\n",
      "F1 Score: 0.7924528301886793\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8421052631578947\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.875\n",
      "\n",
      "Recall: 0.9818181818181818\n",
      "\n",
      "Precision: 0.84375\n",
      "\n",
      "F1 Score: 0.9075630252100839\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7386363636363636\n",
      "\n",
      "Recall: 0.7454545454545455\n",
      "\n",
      "Precision: 0.82\n",
      "\n",
      "F1 Score: 0.780952380952381\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8703703703703703\n",
      "\n",
      "F1 Score: 0.8623853211009175\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7840909090909091\n",
      "\n",
      "Recall: 0.8\n",
      "\n",
      "Precision: 0.8461538461538461\n",
      "\n",
      "F1 Score: 0.8224299065420562\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8068181818181818\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8392857142857143\n",
      "\n",
      "F1 Score: 0.8468468468468467\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6931818181818182\n",
      "\n",
      "Recall: 0.6181818181818182\n",
      "\n",
      "Precision: 0.85\n",
      "\n",
      "F1 Score: 0.7157894736842104\n",
      "\n",
      "[19:44:37] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7840909090909091\n",
      "\n",
      "Recall: 0.8181818181818182\n",
      "\n",
      "Precision: 0.8333333333333334\n",
      "\n",
      "F1 Score: 0.8256880733944955\n",
      "\n",
      "[19:44:37] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.75\n",
      "\n",
      "Recall: 0.7272727272727273\n",
      "\n",
      "Precision: 0.851063829787234\n",
      "\n",
      "F1 Score: 0.7843137254901961\n",
      "\n",
      "-----------------------\n",
      "Top Model:  PassiveAggressiveClassifier(random_state=42)\n",
      "Top F1 Score 0.9075630252100839\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns:\")\n",
    "print(\"normalized:\")\n",
    "print(\"downsample\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection_normalize(X_train_drop_downsample, y_train_drop_downsample, X_test_drop, y_test_drop, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Dropped Columns Not Normalized Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df without dropped columns:\n",
      "not normalized:\n",
      "downsample\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.875\n",
      "\n",
      "F1 Score: 0.8828828828828829\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7045454545454546\n",
      "\n",
      "Recall: 0.8\n",
      "\n",
      "Precision: 0.7457627118644068\n",
      "\n",
      "F1 Score: 0.7719298245614035\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.875\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8666666666666667\n",
      "\n",
      "F1 Score: 0.9043478260869566\n",
      "\n",
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7386363636363636\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.7352941176470589\n",
      "\n",
      "F1 Score: 0.8130081300813008\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.9272727272727272\n",
      "\n",
      "Precision: 0.85\n",
      "\n",
      "F1 Score: 0.8869565217391303\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.625\n",
      "\n",
      "Recall: 0.6363636363636364\n",
      "\n",
      "Precision: 0.7291666666666666\n",
      "\n",
      "F1 Score: 0.6796116504854369\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.625\n",
      "\n",
      "Recall: 0.43636363636363634\n",
      "\n",
      "Precision: 0.9230769230769231\n",
      "\n",
      "F1 Score: 0.5925925925925927\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.5454545454545454\n",
      "\n",
      "Recall: 0.2727272727272727\n",
      "\n",
      "Precision: 1.0\n",
      "\n",
      "F1 Score: 0.42857142857142855\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8068181818181818\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8392857142857143\n",
      "\n",
      "F1 Score: 0.8468468468468467\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8068181818181818\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8392857142857143\n",
      "\n",
      "F1 Score: 0.8468468468468467\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7840909090909091\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8103448275862069\n",
      "\n",
      "F1 Score: 0.831858407079646\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6022727272727273\n",
      "\n",
      "Recall: 0.6\n",
      "\n",
      "Precision: 0.717391304347826\n",
      "\n",
      "F1 Score: 0.6534653465346534\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8068181818181818\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8392857142857143\n",
      "\n",
      "F1 Score: 0.8468468468468467\n",
      "\n",
      "[19:44:38] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7727272727272727\n",
      "\n",
      "Recall: 0.8363636363636363\n",
      "\n",
      "Precision: 0.8070175438596491\n",
      "\n",
      "F1 Score: 0.8214285714285714\n",
      "\n",
      "[19:44:39] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8448275862068966\n",
      "\n",
      "F1 Score: 0.8672566371681416\n",
      "\n",
      "-----------------------\n",
      "Top Model:  LogisticRegression(random_state=42)\n",
      "Top F1 Score 0.9043478260869566\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df without dropped columns:\")\n",
    "print(\"not normalized:\")\n",
    "print(\"downsample\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection(X_train_downsample, y_train_downsample, X_test, y_test, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Dropped Columns Normalized Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df without dropped columns:\n",
      "normalized:\n",
      "downsample\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8545454545454545\n",
      "\n",
      "F1 Score: 0.8545454545454545\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7840909090909091\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8103448275862069\n",
      "\n",
      "F1 Score: 0.831858407079646\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8524590163934426\n",
      "\n",
      "F1 Score: 0.8965517241379309\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8524590163934426\n",
      "\n",
      "F1 Score: 0.8965517241379309\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8571428571428571\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8571428571428571\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.875\n",
      "\n",
      "Recall: 0.9818181818181818\n",
      "\n",
      "Precision: 0.84375\n",
      "\n",
      "F1 Score: 0.9075630252100839\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.847457627118644\n",
      "\n",
      "F1 Score: 0.8771929824561402\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8181818181818182\n",
      "\n",
      "Precision: 0.8823529411764706\n",
      "\n",
      "F1 Score: 0.8490566037735848\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7840909090909091\n",
      "\n",
      "Recall: 0.7818181818181819\n",
      "\n",
      "Precision: 0.86\n",
      "\n",
      "F1 Score: 0.819047619047619\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8545454545454545\n",
      "\n",
      "F1 Score: 0.8545454545454545\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.9272727272727272\n",
      "\n",
      "Precision: 0.85\n",
      "\n",
      "F1 Score: 0.8869565217391303\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7386363636363636\n",
      "\n",
      "Recall: 0.7272727272727273\n",
      "\n",
      "Precision: 0.8333333333333334\n",
      "\n",
      "F1 Score: 0.7766990291262137\n",
      "\n",
      "[19:44:40] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7727272727272727\n",
      "\n",
      "Recall: 0.8363636363636363\n",
      "\n",
      "Precision: 0.8070175438596491\n",
      "\n",
      "F1 Score: 0.8214285714285714\n",
      "\n",
      "[19:44:41] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8448275862068966\n",
      "\n",
      "F1 Score: 0.8672566371681416\n",
      "\n",
      "-----------------------\n",
      "Top Model:  SGDClassifier(random_state=42)\n",
      "Top F1 Score 0.9075630252100839\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df without dropped columns:\")\n",
    "print(\"normalized:\")\n",
    "print(\"downsample\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection_normalize(X_train_downsample, y_train_downsample, X_test, y_test, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropped Columns Not Normalized SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns:\n",
      "not normalized:\n",
      "SMOTE\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.8333333333333334\n",
      "\n",
      "F1 Score: 0.8695652173913043\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8863636363636364\n",
      "\n",
      "Recall: 0.9636363636363636\n",
      "\n",
      "Precision: 0.8688524590163934\n",
      "\n",
      "F1 Score: 0.9137931034482758\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.9272727272727272\n",
      "\n",
      "Precision: 0.85\n",
      "\n",
      "F1 Score: 0.8869565217391303\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8524590163934426\n",
      "\n",
      "F1 Score: 0.8965517241379309\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8448275862068966\n",
      "\n",
      "F1 Score: 0.8672566371681416\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8363636363636363\n",
      "\n",
      "Precision: 0.8846153846153846\n",
      "\n",
      "F1 Score: 0.8598130841121494\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6022727272727273\n",
      "\n",
      "Recall: 0.43636363636363634\n",
      "\n",
      "Precision: 0.8571428571428571\n",
      "\n",
      "F1 Score: 0.5783132530120482\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.4431818181818182\n",
      "\n",
      "Recall: 0.10909090909090909\n",
      "\n",
      "Precision: 1.0\n",
      "\n",
      "F1 Score: 0.19672131147540983\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.875\n",
      "\n",
      "Recall: 0.9272727272727272\n",
      "\n",
      "Precision: 0.8793103448275862\n",
      "\n",
      "F1 Score: 0.902654867256637\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8524590163934426\n",
      "\n",
      "F1 Score: 0.8965517241379309\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.875\n",
      "\n",
      "F1 Score: 0.8828828828828829\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.8620689655172413\n",
      "\n",
      "F1 Score: 0.8849557522123893\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8571428571428571\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "[19:44:42] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.8333333333333334\n",
      "\n",
      "F1 Score: 0.8695652173913043\n",
      "\n",
      "[19:44:42] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9272727272727272\n",
      "\n",
      "Precision: 0.864406779661017\n",
      "\n",
      "F1 Score: 0.8947368421052632\n",
      "\n",
      "-----------------------\n",
      "Top Model:  SVC(random_state=42)\n",
      "Top F1 Score 0.9137931034482758\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns:\")\n",
    "print(\"not normalized:\")\n",
    "print(\"SMOTE\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection(X_train_drop_smote, y_train_drop_smote, X_test_drop, y_test_drop, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropped Columns Normalized SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with dropped columns:\n",
      "normalized:\n",
      "SMOTE\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8596491228070176\n",
      "\n",
      "F1 Score: 0.875\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8545454545454545\n",
      "\n",
      "F1 Score: 0.8545454545454545\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8448275862068966\n",
      "\n",
      "F1 Score: 0.8672566371681416\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8421052631578947\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8421052631578947\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.7954545454545454\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8245614035087719\n",
      "\n",
      "F1 Score: 0.8392857142857144\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 1.0\n",
      "\n",
      "Precision: 0.8088235294117647\n",
      "\n",
      "F1 Score: 0.8943089430894309\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.45454545454545453\n",
      "\n",
      "Recall: 0.12727272727272726\n",
      "\n",
      "Precision: 1.0\n",
      "\n",
      "F1 Score: 0.22580645161290322\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9272727272727272\n",
      "\n",
      "Precision: 0.864406779661017\n",
      "\n",
      "F1 Score: 0.8947368421052632\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.875\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8666666666666667\n",
      "\n",
      "F1 Score: 0.9043478260869566\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.875\n",
      "\n",
      "Recall: 0.9272727272727272\n",
      "\n",
      "Precision: 0.8793103448275862\n",
      "\n",
      "F1 Score: 0.902654867256637\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.847457627118644\n",
      "\n",
      "F1 Score: 0.8771929824561402\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8363636363636363\n",
      "\n",
      "Precision: 0.8846153846153846\n",
      "\n",
      "F1 Score: 0.8598130841121494\n",
      "\n",
      "[19:44:44] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.847457627118644\n",
      "\n",
      "F1 Score: 0.8771929824561402\n",
      "\n",
      "[19:44:44] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.8620689655172413\n",
      "\n",
      "F1 Score: 0.8849557522123893\n",
      "\n",
      "-----------------------\n",
      "Top Model:  AdaBoostClassifier(random_state=42)\n",
      "Top F1 Score 0.9043478260869566\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df with dropped columns:\")\n",
    "print(\"normalized:\")\n",
    "print(\"SMOTE\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection_normalize(X_train_drop_smote, y_train_drop_smote, X_test_drop, y_test_drop, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smote_cols = X_train_smote.columns.tolist()\n",
    "X_train_smote_cols.insert(0, X_train_smote_cols.pop(1))\n",
    "X_train_smote = X_train_smote[X_train_smote_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No  Dropped Columns Not Normalized SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df without dropped columns:\n",
      "not normalized:\n",
      "SMOTE\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8727272727272727\n",
      "\n",
      "F1 Score: 0.8727272727272727\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7045454545454546\n",
      "\n",
      "Recall: 0.7636363636363637\n",
      "\n",
      "Precision: 0.7636363636363637\n",
      "\n",
      "F1 Score: 0.7636363636363637\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.875\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8666666666666667\n",
      "\n",
      "F1 Score: 0.9043478260869566\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.9636363636363636\n",
      "\n",
      "Precision: 0.828125\n",
      "\n",
      "F1 Score: 0.8907563025210083\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8524590163934426\n",
      "\n",
      "F1 Score: 0.8965517241379309\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.6477272727272727\n",
      "\n",
      "Recall: 0.6363636363636364\n",
      "\n",
      "Precision: 0.7608695652173914\n",
      "\n",
      "F1 Score: 0.693069306930693\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7045454545454546\n",
      "\n",
      "Recall: 0.7272727272727273\n",
      "\n",
      "Precision: 0.7843137254901961\n",
      "\n",
      "F1 Score: 0.7547169811320754\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6477272727272727\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.6714285714285714\n",
      "\n",
      "F1 Score: 0.752\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8068181818181818\n",
      "\n",
      "Recall: 0.8545454545454545\n",
      "\n",
      "Precision: 0.8392857142857143\n",
      "\n",
      "F1 Score: 0.8468468468468467\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.819672131147541\n",
      "\n",
      "F1 Score: 0.8620689655172413\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7727272727272727\n",
      "\n",
      "Recall: 0.8363636363636363\n",
      "\n",
      "Precision: 0.8070175438596491\n",
      "\n",
      "F1 Score: 0.8214285714285714\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.6022727272727273\n",
      "\n",
      "Recall: 0.6\n",
      "\n",
      "Precision: 0.717391304347826\n",
      "\n",
      "F1 Score: 0.6534653465346534\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7613636363636364\n",
      "\n",
      "Recall: 0.8\n",
      "\n",
      "Precision: 0.8148148148148148\n",
      "\n",
      "F1 Score: 0.8073394495412846\n",
      "\n",
      "[19:50:22] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.7954545454545454\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8032786885245902\n",
      "\n",
      "F1 Score: 0.8448275862068965\n",
      "\n",
      "[19:50:22] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.819672131147541\n",
      "\n",
      "F1 Score: 0.8620689655172413\n",
      "\n",
      "-----------------------\n",
      "Top Model:  LogisticRegression(random_state=42)\n",
      "Top F1 Score 0.9043478260869566\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df without dropped columns:\")\n",
    "print(\"not normalized:\")\n",
    "print(\"SMOTE\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection(X_train_smote, y_train_smote, X_test, y_test, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Dropped Columns Normalized SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df without dropped columns:\n",
      "normalized:\n",
      "SMOTE\n",
      "RandomForestClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8571428571428571\n",
      "\n",
      "F1 Score: 0.8648648648648648\n",
      "\n",
      "SVC(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7954545454545454\n",
      "\n",
      "Recall: 0.8727272727272727\n",
      "\n",
      "Precision: 0.8135593220338984\n",
      "\n",
      "F1 Score: 0.8421052631578948\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8524590163934426\n",
      "\n",
      "F1 Score: 0.8965517241379309\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cam/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.8620689655172413\n",
      "\n",
      "F1 Score: 0.8849557522123893\n",
      "\n",
      "GaussianNB()\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8596491228070176\n",
      "\n",
      "F1 Score: 0.875\n",
      "\n",
      "KNeighborsClassifier()\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8596491228070176\n",
      "\n",
      "F1 Score: 0.875\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.9818181818181818\n",
      "\n",
      "Precision: 0.8059701492537313\n",
      "\n",
      "F1 Score: 0.8852459016393442\n",
      "\n",
      "PassiveAggressiveClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.4318181818181818\n",
      "\n",
      "Recall: 0.09090909090909091\n",
      "\n",
      "Precision: 1.0\n",
      "\n",
      "F1 Score: 0.16666666666666669\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8522727272727273\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.875\n",
      "\n",
      "F1 Score: 0.8828828828828829\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8305084745762712\n",
      "\n",
      "F1 Score: 0.8596491228070176\n",
      "\n",
      "HistGradientBoostingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8448275862068966\n",
      "\n",
      "F1 Score: 0.8672566371681416\n",
      "\n",
      "GaussianProcessClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Recall: 0.9454545454545454\n",
      "\n",
      "Precision: 0.8524590163934426\n",
      "\n",
      "F1 Score: 0.8965517241379309\n",
      "\n",
      "BaggingClassifier(random_state=42)\n",
      "-----------------------\n",
      "Accuracy: 0.7840909090909091\n",
      "\n",
      "Recall: 0.7818181818181819\n",
      "\n",
      "Precision: 0.86\n",
      "\n",
      "F1 Score: 0.819047619047619\n",
      "\n",
      "[19:50:41] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8295454545454546\n",
      "\n",
      "Recall: 0.8909090909090909\n",
      "\n",
      "Precision: 0.8448275862068966\n",
      "\n",
      "F1 Score: 0.8672566371681416\n",
      "\n",
      "[19:50:41] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBRFClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "                colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain',\n",
      "                interaction_constraints='', max_delta_step=0, max_depth=6,\n",
      "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "                n_estimators=100, n_jobs=4, num_parallel_tree=100,\n",
      "                objective='binary:logistic', random_state=42, reg_alpha=0,\n",
      "                scale_pos_weight=1, tree_method='exact',\n",
      "                use_label_encoder=False, validate_parameters=1, verbosity=None)\n",
      "-----------------------\n",
      "Accuracy: 0.8409090909090909\n",
      "\n",
      "Recall: 0.9090909090909091\n",
      "\n",
      "Precision: 0.847457627118644\n",
      "\n",
      "F1 Score: 0.8771929824561402\n",
      "\n",
      "-----------------------\n",
      "Top Model:  LogisticRegression(random_state=42)\n",
      "Top F1 Score 0.8965517241379309\n"
     ]
    }
   ],
   "source": [
    "estimator = None\n",
    "top_model = None\n",
    "top_f1 = 0\n",
    "print(\"df without dropped columns:\")\n",
    "print(\"normalized:\")\n",
    "print(\"SMOTE\")\n",
    "for estimator in estimator_lst:\n",
    "    modelSelection = model_selection_normalize(X_train_smote, y_train_smote, X_test, y_test, estimator)\n",
    "    print(estimator)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Accuracy: \" + str(modelSelection[0]) + \"\\n\")\n",
    "    print(\"Recall: \" + str(modelSelection[1])+ \"\\n\")\n",
    "    print(\"Precision: \" + str(modelSelection[2])+ \"\\n\")\n",
    "    print(\"F1 Score: \" + str(modelSelection[3])+ \"\\n\")\n",
    "    if modelSelection[3] > top_f1:\n",
    "        top_f1 = modelSelection[3]\n",
    "        top_model = estimator\n",
    "        \n",
    "print(\"-----------------------\")\n",
    "print(\"Top Model: \", top_model)\n",
    "print(\"Top F1 Score\", top_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passive Aggressive Columns Dropped Not Normalized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8522727272727273\n",
      "Recall: 0.9454545454545454\n",
      "Precision: 0.8387096774193549\n",
      "F1 Score: 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "clf = PassiveAggressiveClassifier(random_state=42)\n",
    "clf.fit(X_train_drop, y_train_drop)\n",
    "\n",
    "predictions = clf.predict(X_test_drop)\n",
    "actual = y_test_drop\n",
    "\n",
    "\n",
    "print(\"Accuracy: \" + str(clf.score(X_test_drop, y_test_drop)))\n",
    "print(\"Recall: \" + str(recall_score(actual, predictions)))\n",
    "print(\"Precision: \" + str(precision_score(actual, predictions)))\n",
    "print(\"F1 Score: \" + str(f1_score(actual, predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(clf, X_test_drop, y_test_drop)  \n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
